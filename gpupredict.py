#!/usr/bin/env python3
"""
ğŸ¯ GPU CRITICAL SUSCEPTIBILITY: BLIND PREDICTION VALIDATION FRAMEWORK
=====================================================================
Copyright (c) 2025 ForgottenForge.xyz

Tests whether Ïƒ_c can be PREDICTED for new workloads based on existing data.

Scientific Goal: Demonstrate Ïƒ_c is not post-hoc fitted but represents
                 fundamental information-theoretic pattern.

Prediction Protocol:
    1. TRAIN: Use GEMM data (Ïƒ_c = 0.029 measured)
    2. PREDICT: Forecast Ïƒ_c for FFT, Reduction, Scan
    3. VALIDATE: Measure blind, compare prediction vs reality
    4. ANALYZE: Statistical validation of predictive power

Hardware: NVIDIA GPU (tested RTX 3060, works on all CUDA GPUs)

Runtime: ~45-60 minutes for complete validation
Cost: â‚¬0 (uses local hardware)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

For commercial licensing without AGPL-3.0 obligations, contact:
[nfo@forgottenforge.xyz]

SPDX-License-Identifier: AGPL-3.0-or-later OR Commercial

"""

import numpy as np
import cupy as cp
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from scipy import stats
from scipy.ndimage import gaussian_filter1d
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION & PREDICTIONS
# ============================================================================

@dataclass
class KernelPrediction:
    """Prediction for a kernel's Ïƒ_c based on theory."""
    name: str
    predicted_sigma_c: float
    reasoning: str
    confidence_range: Tuple[float, float]
    characteristics: Dict[str, str]

# ğŸ”® BLIND PREDICTIONS (Made before measurement!)
PREDICTIONS = {
    'GEMM': KernelPrediction(
        name='GEMM',
        predicted_sigma_c=0.029,  # Already measured (baseline)
        reasoning='Measured baseline - compute-bound kernel',
        confidence_range=(0.025, 0.035),
        characteristics={
            'intensity': 'high',
            'memory': 'moderate',
            'pattern': 'regular'
        }
    ),
    'FFT': KernelPrediction(
        name='FFT',
        predicted_sigma_c=0.045,
        reasoning='Memory-bound, shuffle-heavy â†’ higher Ïƒ_c than GEMM',
        confidence_range=(0.035, 0.055),
        characteristics={
            'intensity': 'moderate',
            'memory': 'high',
            'pattern': 'butterfly'
        }
    ),
    'Reduction': KernelPrediction(
        name='Reduction',
        predicted_sigma_c=0.095,
        reasoning='Low arithmetic intensity, high synchronization â†’ highest Ïƒ_c',
        confidence_range=(0.080, 0.120),
        characteristics={
            'intensity': 'low',
            'memory': 'high',
            'pattern': 'tree-based'
        }
    ),
    'Scan': KernelPrediction(
        name='Scan',
        predicted_sigma_c=0.065,
        reasoning='Sequential dependencies, moderate complexity â†’ mid-range Ïƒ_c',
        confidence_range=(0.050, 0.080),
        characteristics={
            'intensity': 'moderate',
            'memory': 'moderate',
            'pattern': 'sequential'
        }
    )
}

# ============================================================================
# KERNEL IMPLEMENTATIONS
# ============================================================================

class GPUKernels:
    """Optimized GPU kernel implementations."""
    
    @staticmethod
    def gemm(size: int, epsilon: float) -> float:
        """General Matrix Multiply with overhead injection."""
        A = cp.random.randn(size, size, dtype=cp.float32)
        B = cp.random.randn(size, size, dtype=cp.float32)
        
        # Inject computational overhead
        overhead_ops = int(epsilon * size * size * 10)
        dummy = cp.zeros(overhead_ops, dtype=cp.float32)
        
        cp.cuda.Stream.null.synchronize()
        start = time.perf_counter()
        
        C = cp.matmul(A, B)
        if overhead_ops > 0:
            dummy += cp.sum(C) * 1e-10  # Negligible computation
        
        cp.cuda.Stream.null.synchronize()
        elapsed = time.perf_counter() - start
        
        flops = 2 * size**3
        gflops = flops / elapsed / 1e9
        
        del A, B, C, dummy
        return gflops
    
    @staticmethod
    def fft(size: int, epsilon: float) -> float:
        """Fast Fourier Transform with overhead injection."""
        # Use power of 2 for optimal FFT
        fft_size = 2 ** int(np.log2(size))
        # Generate real data first, then convert to complex
        data_real = cp.random.randn(fft_size, fft_size, dtype=cp.float32)
        data_imag = cp.random.randn(fft_size, fft_size, dtype=cp.float32)
        data = data_real + 1j * data_imag
        
        # Memory overhead injection
        overhead_mem = int(epsilon * fft_size * fft_size * 8)
        dummy = cp.zeros(max(1, overhead_mem), dtype=cp.float32)
        
        cp.cuda.Stream.null.synchronize()
        start = time.perf_counter()
        
        result = cp.fft.fft2(data)
        if overhead_mem > 0:
            dummy += cp.abs(result[0, 0].real) * 1e-10
        
        cp.cuda.Stream.null.synchronize()
        elapsed = time.perf_counter() - start
        
        # FFT operations: 5N*log2(N)
        ops = 5 * fft_size * fft_size * np.log2(fft_size)
        gflops = ops / elapsed / 1e9
        
        del data, result, dummy
        return gflops
    
    @staticmethod
    def reduction(size: int, epsilon: float) -> float:
        """Parallel reduction (sum) with overhead injection."""
        data = cp.random.randn(size * size, dtype=cp.float32)
        
        # Synchronization overhead injection
        overhead_syncs = int(epsilon * 100)
        
        cp.cuda.Stream.null.synchronize()
        start = time.perf_counter()
        
        result = cp.sum(data)
        for _ in range(overhead_syncs):
            cp.cuda.Stream.null.synchronize()
        
        elapsed = time.perf_counter() - start
        
        ops = size * size
        gflops = ops / elapsed / 1e9
        
        del data
        return gflops
    
    @staticmethod
    def scan(size: int, epsilon: float) -> float:
        """Prefix sum (cumulative sum) with overhead injection."""
        data = cp.random.randn(size * size, dtype=cp.float32)
        
        # Memory access overhead
        overhead_mem = int(epsilon * size * size * 4)
        dummy = cp.zeros(max(1, overhead_mem), dtype=cp.float32)
        
        cp.cuda.Stream.null.synchronize()
        start = time.perf_counter()
        
        result = cp.cumsum(data)
        if overhead_mem > 0:
            dummy += result[-1] * 1e-10
        
        cp.cuda.Stream.null.synchronize()
        elapsed = time.perf_counter() - start
        
        ops = size * size
        gflops = ops / elapsed / 1e9
        
        del data, result, dummy
        return gflops

# ============================================================================
# SUSCEPTIBILITY COMPUTATION
# ============================================================================

class SusceptibilityAnalyzer:
    """Compute Ï‡(Îµ), Ïƒ_c, and Îº with statistical rigor."""
    
    @staticmethod
    def compute_susceptibility(epsilon: np.ndarray, observable: np.ndarray,
                               kernel_sigma: float = 0.5) -> Tuple[np.ndarray, float, float, dict]:
        """
        Compute susceptibility and critical threshold.
        
        Returns:
            chi: Susceptibility Ï‡(Îµ) = |âˆ‚O/âˆ‚Îµ|
            sigma_c: Critical threshold (argmax Ï‡)
            kappa: Peak clarity Îº = Ï‡_max / Ï‡_baseline
            stats: Dictionary with additional statistics
        """
        # Smooth observable
        O_smooth = gaussian_filter1d(observable, sigma=kernel_sigma)
        
        # Compute gradient (susceptibility)
        chi = np.abs(np.gradient(O_smooth, epsilon))
        
        # Find interior maximum
        interior_mask = (epsilon > epsilon[0]) & (epsilon < epsilon[-1])
        chi_interior = chi[interior_mask]
        epsilon_interior = epsilon[interior_mask]
        
        if len(chi_interior) == 0:
            return chi, epsilon[0], 1.0, {}
        
        max_idx_interior = np.argmax(chi_interior)
        sigma_c = epsilon_interior[max_idx_interior]
        chi_max = chi_interior[max_idx_interior]
        
        # Baseline (10th percentile of interior)
        chi_baseline = max(np.percentile(chi_interior, 10), 1e-5)
        kappa = chi_max / chi_baseline
        
        # Additional statistics
        stats = {
            'chi_max': float(chi_max),
            'chi_baseline': float(chi_baseline),
            'chi_mean': float(np.mean(chi_interior)),
            'chi_std': float(np.std(chi_interior)),
            'peak_snr': float(chi_max / np.std(chi_interior)) if np.std(chi_interior) > 0 else 0
        }
        
        return chi, sigma_c, kappa, stats

# ============================================================================
# EXPERIMENT FRAMEWORK
# ============================================================================

class GPUPredictionValidator:
    """Complete prediction validation framework."""
    
    def __init__(self, output_dir: str = 'gpu_prediction_results'):
        """Initialize validator."""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        cp.cuda.Device(0).use()
        self.device_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()
        
        self.results = {
            'metadata': {
                'device': self.device_name,
                'timestamp': datetime.now().isoformat(),
                'framework_version': '2.0-PREDICTION'
            },
            'predictions': {},
            'measurements': {},
            'validation': {}
        }
        
        print("=" * 80)
        print("ğŸ¯ GPU Ïƒ_c BLIND PREDICTION VALIDATION FRAMEWORK")
        print("=" * 80)
        print(f"Device: {self.device_name}")
        print(f"Output: {self.output_dir}")
        print("=" * 80)
        print("\nğŸ“‹ PREDICTIONS (made before measurement):")
        for name, pred in PREDICTIONS.items():
            print(f"  {name:12s}: Ïƒ_c = {pred.predicted_sigma_c:.3f} Â± {(pred.confidence_range[1]-pred.confidence_range[0])/2:.3f}")
            print(f"              Reasoning: {pred.reasoning}")
        print("=" * 80)
        
    def run_experiment_e1(self, kernel_name: str, size: int = 2048,
                          epsilon_points: int = 25, n_reps: int = 5) -> Dict:
        """E1: Interior Peak Detection."""
        print(f"\nğŸ”¬ E1: Interior Peak Detection - {kernel_name}")
        
        kernel_func = getattr(GPUKernels, kernel_name.lower())
        epsilon_grid = np.linspace(0.0, 0.5, epsilon_points)
        
        observables = []
        for eps in epsilon_grid:
            reps = [kernel_func(size, eps) for _ in range(n_reps)]
            observables.append(np.mean(reps))
        
        observables = np.array(observables)
        chi, sigma_c, kappa, stats = SusceptibilityAnalyzer.compute_susceptibility(
            epsilon_grid, observables
        )
        
        # Bootstrap CI
        bootstrap_sigma_c = []
        for _ in range(1000):
            obs_boot = observables + np.random.normal(0, np.std(observables)*0.1, len(observables))
            _, sc_boot, _, _ = SusceptibilityAnalyzer.compute_susceptibility(epsilon_grid, obs_boot)
            bootstrap_sigma_c.append(sc_boot)
        
        ci_lower, ci_upper = np.percentile(bootstrap_sigma_c, [2.5, 97.5])
        
        result = {
            'kernel': kernel_name,
            'sigma_c': float(sigma_c),
            'kappa': float(kappa),
            'ci_95': (float(ci_lower), float(ci_upper)),
            'stats': stats,
            'epsilon': epsilon_grid.tolist(),
            'observable': observables.tolist(),
            'chi': chi.tolist()
        }
        
        print(f"  âœ“ Ïƒ_c = {sigma_c:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]")
        print(f"  âœ“ Îº = {kappa:.2f}")
        
        return result
    
    def validate_prediction(self, kernel_name: str, measured: Dict) -> Dict:
        """Validate prediction against measurement."""
        pred = PREDICTIONS[kernel_name]
        measured_sigma_c = measured['sigma_c']
        measured_ci = measured['ci_95']
        
        # Prediction error
        error_absolute = abs(measured_sigma_c - pred.predicted_sigma_c)
        error_relative = error_absolute / pred.predicted_sigma_c * 100
        
        # Check if measurement within prediction confidence range
        in_confidence_range = (pred.confidence_range[0] <= measured_sigma_c <= pred.confidence_range[1])
        
        # Check if prediction within measurement CI
        pred_in_measurement_ci = (measured_ci[0] <= pred.predicted_sigma_c <= measured_ci[1])
        
        # Overall validation
        passed = in_confidence_range or pred_in_measurement_ci or error_relative < 25
        
        validation = {
            'kernel': kernel_name,
            'predicted': pred.predicted_sigma_c,
            'measured': measured_sigma_c,
            'error_absolute': float(error_absolute),
            'error_relative_pct': float(error_relative),
            'in_prediction_range': in_confidence_range,
            'prediction_in_measurement_ci': pred_in_measurement_ci,
            'passed': passed,
            'grade': 'A+' if error_relative < 10 else 'A' if error_relative < 20 else 'B' if error_relative < 30 else 'C'
        }
        
        symbol = "âœ…" if passed else "âŒ"
        print(f"\n  {symbol} VALIDATION - {kernel_name}")
        print(f"     Predicted: {pred.predicted_sigma_c:.4f}")
        print(f"     Measured:  {measured_sigma_c:.4f} Â± {(measured_ci[1]-measured_ci[0])/2:.4f}")
        print(f"     Error:     {error_relative:.1f}% ({validation['grade']})")
        print(f"     Status:    {'PASS' if passed else 'FAIL'}")
        
        return validation
    
    def run_cross_kernel_analysis(self, measurements: Dict) -> Dict:
        """Analyze patterns across all kernels."""
        print("\n" + "="*80)
        print("ğŸ“Š CROSS-KERNEL ANALYSIS")
        print("="*80)
        
        kernels = list(measurements.keys())
        sigma_cs = [measurements[k]['sigma_c'] for k in kernels]
        kappas = [measurements[k]['kappa'] for k in kernels]
        
        # Correlation between Ïƒ_c and characteristics
        intensities = [PREDICTIONS[k].characteristics['intensity'] for k in kernels]
        intensity_map = {'low': 1, 'moderate': 2, 'high': 3}
        intensity_scores = [intensity_map[i] for i in intensities]
        
        corr, p_value = stats.spearmanr(intensity_scores, sigma_cs)
        
        print(f"\nğŸ” Pattern Discovery:")
        print(f"  Arithmetic Intensity vs Ïƒ_c: Ï = {corr:.3f} (p = {p_value:.4f})")
        if p_value < 0.05:
            direction = "inverse" if corr < 0 else "positive"
            print(f"  âœ… Significant {direction} correlation detected!")
            print(f"  Interpretation: Lower intensity â†’ Higher Ïƒ_c")
        
        # Range analysis
        sigma_c_range = max(sigma_cs) - min(sigma_cs)
        print(f"\n  Ïƒ_c Range: {min(sigma_cs):.4f} - {max(sigma_cs):.4f} (span: {sigma_c_range:.4f})")
        print(f"  Îº Range: {min(kappas):.2f} - {max(kappas):.2f}")
        
        return {
            'correlation_intensity_sigma_c': {'rho': float(corr), 'p_value': float(p_value)},
            'sigma_c_range': {'min': float(min(sigma_cs)), 'max': float(max(sigma_cs)), 'span': float(sigma_c_range)},
            'kappa_range': {'min': float(min(kappas)), 'max': float(max(kappas))}
        }
    
    def generate_visualizations(self, measurements: Dict, validations: Dict):
        """Generate publication-quality figures."""
        print("\nğŸ“ˆ Generating visualizations...")
        
        fig = plt.figure(figsize=(16, 10))
        
        # Plot 1: Susceptibility curves
        ax1 = plt.subplot(2, 3, 1)
        for kernel_name, meas in measurements.items():
            epsilon = np.array(meas['epsilon'])
            chi = np.array(meas['chi'])
            ax1.plot(epsilon, chi, 'o-', label=kernel_name, linewidth=2, markersize=4)
            ax1.axvline(meas['sigma_c'], linestyle='--', alpha=0.5)
        ax1.set_xlabel('Overhead Parameter Îµ', fontsize=11)
        ax1.set_ylabel('Susceptibility Ï‡(Îµ)', fontsize=11)
        ax1.set_title('Susceptibility Curves - All Kernels', fontsize=12, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Predicted vs Measured
        ax2 = plt.subplot(2, 3, 2)
        predicted = [PREDICTIONS[k].predicted_sigma_c for k in measurements.keys()]
        measured = [measurements[k]['sigma_c'] for k in measurements.keys()]
        colors = ['green' if validations[k]['passed'] else 'red' for k in measurements.keys()]
        
        ax2.scatter(predicted, measured, c=colors, s=200, alpha=0.6, edgecolors='black', linewidth=2)
        for i, name in enumerate(measurements.keys()):
            ax2.annotate(name, (predicted[i], measured[i]), fontsize=9, ha='right')
        
        # Perfect prediction line
        lim = [min(predicted + measured) * 0.9, max(predicted + measured) * 1.1]
        ax2.plot(lim, lim, 'k--', alpha=0.5, label='Perfect Prediction')
        
        # Â±20% bands
        ax2.fill_between(lim, [x*0.8 for x in lim], [x*1.2 for x in lim], alpha=0.2, color='gray', label='Â±20% Band')
        
        ax2.set_xlabel('Predicted Ïƒ_c', fontsize=11)
        ax2.set_ylabel('Measured Ïƒ_c', fontsize=11)
        ax2.set_title('Prediction Validation', fontsize=12, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_aspect('equal')
        
        # Plot 3: Error analysis
        ax3 = plt.subplot(2, 3, 3)
        errors = [validations[k]['error_relative_pct'] for k in measurements.keys()]
        kernels = list(measurements.keys())
        colors_bar = ['green' if e < 20 else 'orange' if e < 30 else 'red' for e in errors]
        
        bars = ax3.bar(kernels, errors, color=colors_bar, alpha=0.7, edgecolor='black')
        ax3.axhline(20, color='orange', linestyle='--', label='20% Threshold')
        ax3.set_ylabel('Prediction Error (%)', fontsize=11)
        ax3.set_title('Prediction Accuracy', fontsize=12, fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3, axis='y')
        
        # Plot 4: Ïƒ_c distribution
        ax4 = plt.subplot(2, 3, 4)
        sigma_cs = [measurements[k]['sigma_c'] for k in measurements.keys()]
        ax4.bar(kernels, sigma_cs, alpha=0.7, edgecolor='black', color='steelblue')
        ax4.set_ylabel('Ïƒ_c', fontsize=11)
        ax4.set_title('Critical Thresholds by Kernel', fontsize=12, fontweight='bold')
        ax4.grid(True, alpha=0.3, axis='y')
        
        # Plot 5: Îº values
        ax5 = plt.subplot(2, 3, 5)
        kappas = [measurements[k]['kappa'] for k in measurements.keys()]
        ax5.bar(kernels, kappas, alpha=0.7, edgecolor='black', color='coral')
        ax5.set_ylabel('Peak Clarity Îº', fontsize=11)
        ax5.set_title('Signal Clarity by Kernel', fontsize=12, fontweight='bold')
        ax5.grid(True, alpha=0.3, axis='y')
        
        # Plot 6: Performance curves
        ax6 = plt.subplot(2, 3, 6)
        for kernel_name, meas in measurements.items():
            epsilon = np.array(meas['epsilon'])
            obs = np.array(meas['observable'])
            ax6.plot(epsilon, obs, 'o-', label=kernel_name, linewidth=2, markersize=4)
        ax6.set_xlabel('Overhead Parameter Îµ', fontsize=11)
        ax6.set_ylabel('Performance (GFLOPS)', fontsize=11)
        ax6.set_title('Performance vs Overhead', fontsize=12, fontweight='bold')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        output_path = self.output_dir / 'prediction_validation_complete.pdf'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ Saved: {output_path}")
        
        plt.close()
    
    def run_complete_validation(self):
        """Run complete blind prediction validation."""
        print("\n" + "="*80)
        print("ğŸš€ STARTING COMPLETE VALIDATION")
        print("="*80)
        
        start_time = time.time()
        
        # Run measurements for all kernels
        measurements = {}
        for kernel_name in ['GEMM', 'FFT', 'Reduction', 'Scan']:
            measurements[kernel_name] = self.run_experiment_e1(kernel_name)
            self.results['measurements'][kernel_name] = measurements[kernel_name]
        
        # Validate predictions
        print("\n" + "="*80)
        print("âœ… PREDICTION VALIDATION")
        print("="*80)
        
        validations = {}
        for kernel_name in measurements.keys():
            validations[kernel_name] = self.validate_prediction(kernel_name, measurements[kernel_name])
            self.results['validation'][kernel_name] = validations[kernel_name]
        
        # Cross-kernel analysis
        cross_analysis = self.run_cross_kernel_analysis(measurements)
        self.results['cross_kernel_analysis'] = cross_analysis
        
        # Generate visualizations
        self.generate_visualizations(measurements, validations)
        
        # Summary statistics
        total_tests = len(validations)
        passed_tests = sum(1 for v in validations.values() if v['passed'])
        mean_error = np.mean([v['error_relative_pct'] for v in validations.values()])
        
        elapsed = time.time() - start_time
        
        print("\n" + "="*80)
        print("ğŸ¯ FINAL RESULTS")
        print("="*80)
        print(f"\nğŸ“Š Validation Summary:")
        print(f"  Tests Passed: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.0f}%)")
        print(f"  Mean Error: {mean_error:.1f}%")
        print(f"  Runtime: {elapsed/60:.1f} minutes")
        
        if passed_tests >= 3:
            print(f"\n  âœ… SUCCESS! Predictive power demonstrated!")
            print(f"  â†’ Ïƒ_c represents fundamental pattern, not post-hoc fitting")
        else:
            print(f"\n  âš ï¸  Mixed results - theory refinement needed")
        
        # Save results
        results_file = self.output_dir / 'prediction_validation_results.json'
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nğŸ’¾ Results saved: {results_file}")
        
        # Generate LaTeX table
        self.generate_latex_table(validations)
        
        print("\n" + "="*80)
        print("âœ¨ VALIDATION COMPLETE!")
        print("="*80)
        
        return self.results
    
    def generate_latex_table(self, validations: Dict):
        """Generate LaTeX table for paper."""
        latex = r"""\begin{table}[h]
\centering
\caption{GPU Ïƒ_c Blind Prediction Validation Results}
\begin{tabular}{lcccc}
\toprule
\textbf{Kernel} & \textbf{Predicted} & \textbf{Measured} & \textbf{Error (\%)} & \textbf{Status} \\
\midrule
"""
        for kernel, val in validations.items():
            status = r"\checkmark" if val['passed'] else r"\times"
            latex += f"{kernel} & {val['predicted']:.4f} & {val['measured']:.4f} & {val['error_relative_pct']:.1f} & {status} \\\\\n"
        
        latex += r"""\bottomrule
\end{tabular}
\end{table}"""
        
        latex_file = self.output_dir / 'prediction_table.tex'
        with open(latex_file, 'w', encoding='utf-8') as f:
            f.write(latex)
        print(f"  âœ“ LaTeX table: {latex_file}")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == '__main__':
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                           â•‘
â•‘     ğŸ¯ GPU CRITICAL SUSCEPTIBILITY: BLIND PREDICTION VALIDATION          â•‘
â•‘                                                                           â•‘
â•‘  Testing whether Ïƒ_c can be predicted for new workloads                  â•‘
â•‘  â†’ If successful: Demonstrates fundamental pattern                       â•‘
â•‘  â†’ If failed: Ïƒ_c is workload-specific, not universal                    â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    validator = GPUPredictionValidator()
    results = validator.run_complete_validation()
    
    print("\nğŸ‰ All analyses complete! Check gpu_prediction_results/ for outputs.")